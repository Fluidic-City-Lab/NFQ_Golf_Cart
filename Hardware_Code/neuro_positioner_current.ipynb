{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import_SteerboxEnv = Method to start the environment\n",
    "\n",
    "Setting up a custom OpenAI Gym environment by inheriting gym.Env\n",
    "\n",
    "Actual environment is class SteerboxEnv: \n",
    "    A. Must define action space and observation space in the constructor:\n",
    "    \n",
    "    B. Must implement theree methods as part of gym interface: \n",
    "        1. step\n",
    "        2. reset \n",
    "        3. render\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def import_SteerboxEnv():\n",
    "    import os\n",
    "    import time\n",
    "    import math\n",
    "    import serial\n",
    "    import struct\n",
    "    \n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    from typing import Callable, List, Tuple\n",
    "\n",
    "    import gym\n",
    "    from gym import logger, spaces\n",
    "    from gym.utils import seeding\n",
    "\n",
    "    class SteerboxEnv(gym.Env):\n",
    "        def __init__(self, mode=\"train\"):\n",
    "            \n",
    "            \"\"\"\n",
    "            Initialization param for Environment\n",
    "            train and eval: just changes values of max_steps to 100 and 300 resp.\n",
    "            \n",
    "            \"\"\"\n",
    "            self.set_mode(mode)\n",
    "            \n",
    "            self.pos_success_range = 0.1 #\n",
    "            self.pos_threshold = 0.5 #\n",
    "            self.c_trans = 0.01 #\n",
    "\n",
    "            \"\"\"\n",
    "            Used later in the obeservation space:\n",
    "                Essentially, an array with 4 elements: defines that we can observe 4 things.\n",
    "                What do there 4 things mean physically?\n",
    "                    1. Position Threshold: \n",
    "                    2. \n",
    "                    3. \n",
    "                    4. \n",
    "            \"\"\"\n",
    "            high = np.array([self.pos_threshold * 1.5,#position of the wheel\n",
    "                    np.finfo(np.float32).max, # Machine dependent max value for a float32 datatype\n",
    "                    np.finfo(np.float32).max,\n",
    "                    np.finfo(np.float32).max,])\n",
    "\n",
    "            \"\"\"\n",
    "            Action: Discrete variable that can take one of two values\n",
    "            Observation: Lowest accepted value = - high, Highest accepted value = + high\n",
    "                        : Box = real valued quantity that can lie inside the range\n",
    "            \n",
    "            \"\"\"\n",
    "            self.action_space = spaces.Discrete(2)\n",
    "            self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "            self.seed() #does nothing\n",
    "            self.viewer = None\n",
    "            self.state = None\n",
    "\n",
    "            self.voltage_mag = 0.8\n",
    "            self.last_voltage = 0\n",
    "            \n",
    "            for f in os.listdir(\"/dev\"): # find the arduino's serial port\n",
    "                if f.startswith(\"ttyUSB\"):\n",
    "                    p = \"/dev/\"+f\n",
    "                    \n",
    "            self.ser = serial.Serial(p, baudrate=115200, timeout=0.5)\n",
    "            self._interact(0, reset=True, ignore_powerup=True)\n",
    "        \n",
    "        \n",
    "        def set_mode(self, mode):\n",
    "            assert mode in [\"train\", \"eval\"]\n",
    "            self.mode = mode\n",
    "            self.max_steps = 100 if mode == \"train\" else 300\n",
    "\n",
    "        def seed(self, seed=None):\n",
    "            pass\n",
    "        \n",
    "        \"\"\"\n",
    "        Params: \n",
    "            voltage\n",
    "            reset\n",
    "            ignore_powerup\n",
    "            \n",
    "        Return: \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # read the current position of the wheel, then send the given voltage command\n",
    "        def _interact(self, voltage, reset=False, ignore_powerup=False):\n",
    "            \n",
    "            if reset:\n",
    "                voltage = 0\n",
    "                self.last_voltage = 0\n",
    "                \n",
    "                # tell hardware to stop a bunch\n",
    "                self.ser.write(b'\\x00\\x00\\x00\\x00')\n",
    "                # clear anything out of the serial receive buffer\n",
    "                \n",
    "                time.sleep(0.1) # ensure hardware times out\n",
    "                self.ser.reset_input_buffer()\n",
    "\n",
    "                # tell it to stop again now that we know it's alive\n",
    "                self.ser.write(b'\\x00')\n",
    "                \n",
    "                # wait for the updated status\n",
    "                _, _, error = struct.unpack(\"<hBB\", self.ser.read(4))\n",
    "                \n",
    "                if not ignore_powerup and error == 0x81:\n",
    "                    raise Exception(\"hardware reset itself unexpectedly\")\n",
    "                # acknowledge the error\n",
    "                \n",
    "                self.ser.write(b'\\x80')\n",
    "\n",
    "            this_pos, last_quadrature_errors, last_time_ms = struct.unpack(\"<hBB\", self.ser.read(4))\n",
    "            \n",
    "            if last_time_ms & 0x80: # error state\n",
    "                raise Exception(\"error type: \"+str(last_time_ms & 0x7F))\n",
    "                \n",
    "            if last_quadrature_errors > 1 or (last_time_ms > 20 and last_time_ms < 127):\n",
    "                print(\"oh NO\", last_quadrature_errors, last_time_ms)\n",
    "                \n",
    "            this_pos = float(this_pos)/(4*2802) # convert encoder counts to fractions of a circle\n",
    "            \n",
    "            if abs(this_pos) > 1.5: # wheel is rotated too much, stop the experiment before damage\n",
    "                self.ser.write(b'\\x00\\x00\\x00\\x00')\n",
    "                raise Exception(\"TOO FAR!\")\n",
    "            \n",
    "            if self.last_voltage < voltage: # move motor voltage closer to target voltage\n",
    "                self.last_voltage += min(0.05, voltage-self.last_voltage)\n",
    "            else:\n",
    "                self.last_voltage -= min(0.05, self.last_voltage-voltage)\n",
    "            \n",
    "            # send motor voltage and direction\n",
    "            if self.last_voltage >= 0:\n",
    "                cmd = int(self.last_voltage*127)\n",
    "            else:\n",
    "                cmd = 128 + int(-self.last_voltage*127)\n",
    "            self.ser.write(bytes([cmd]))\n",
    "\n",
    "            return this_pos\n",
    "\n",
    "        def _compute_next_state(self, state, action):\n",
    "            voltage = self.voltage_mag if action == 1 else -self.voltage_mag\n",
    "            \n",
    "            voltage *= 14/15 # scale voltage down a little to avoid too wild of an action\n",
    "            \n",
    "            v = self.last_voltage\n",
    "            this_pos = self._interact(voltage)\n",
    "            \n",
    "            state = (this_pos, this_pos-state[0], v, self.last_action)\n",
    "            self.last_action = action\n",
    "            \n",
    "            return state\n",
    "        \n",
    "        \"\"\"\n",
    "        Execute one time step within an environment.\n",
    "        \n",
    "        \"\"\"\n",
    "        def step(self, action):\n",
    "            assert self.action_space.contains(action), \"%r (%s) invalid\" % (\n",
    "                action,\n",
    "                type(action),\n",
    "            )\n",
    "            self.state = self._compute_next_state(self.state, action)\n",
    "            pos, vel, _, _ = self.state\n",
    "\n",
    "            self.episode_step += 1\n",
    "\n",
    "            # Forbidden States (S-)\n",
    "            if (\n",
    "                pos < -self.pos_threshold\n",
    "                or pos > self.pos_threshold\n",
    "            ):\n",
    "                done = True\n",
    "                cost = 1\n",
    "            # Goal States (S+)\n",
    "            elif (\n",
    "                -self.pos_success_range < pos < self.pos_success_range\n",
    "                and -0.01 < vel < 0.01\n",
    "            ):\n",
    "                done = False\n",
    "                cost = 0\n",
    "            else:\n",
    "                done = False\n",
    "                cost = self.c_trans\n",
    "\n",
    "            # Check for time limit\n",
    "            info = {\"time_limit\": self.episode_step >= self.max_steps}\n",
    "\n",
    "            return np.array(self.state), cost, done, info\n",
    "\n",
    "        \"\"\"\n",
    "        Reset the state of the environment so that we get an initial state\n",
    "        \"\"\"\n",
    "        def reset(self):\n",
    "            self.last_action = 0\n",
    "\n",
    "            self._interact(0, reset=True)\n",
    "            time.sleep(0.5)\n",
    "            self._interact(0, reset=True)\n",
    "            pos = ppos = self._interact(0)\n",
    "            \n",
    "            # rotate wheel to random initial position\n",
    "            goal = ((2*random.random())-1)*0.5*self.pos_threshold\n",
    "            \n",
    "            if pos > goal:\n",
    "                while pos > goal+0.1:\n",
    "                    pos = self._interact(-0.7)\n",
    "            else:\n",
    "                while pos < goal-0.1:\n",
    "                    pos = self._interact(0.7)\n",
    "\n",
    "            self._interact(0, reset=True)\n",
    "            time.sleep(0.5)\n",
    "            pos = self._interact(0, reset=True)\n",
    "            print(\"goal:\", ppos, \"->\", goal, \"~\", pos)\n",
    "\n",
    "            self.episode_step = 0\n",
    "            \n",
    "            self.state = (pos, 0, 0, 0)\n",
    "\n",
    "            return np.array(self.state)\n",
    "\n",
    "        # No virtual environment to render\n",
    "        def render(self, mode=\"human\"):\n",
    "            pass\n",
    "\n",
    "        def close(self):\n",
    "            self.ser.close()\n",
    "\n",
    "        def get_goal_pattern_set(self, size: int = 200):\n",
    "            \"\"\"Use hint-to-goal heuristic to clamp network output.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            size : int\n",
    "                The size of the goal pattern set to generate.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            pattern_set : tuple of np.ndarray\n",
    "                Pattern set to train the NFQ network.\n",
    "\n",
    "            \"\"\"\n",
    "            goal_state_action_b = [\n",
    "                np.array(\n",
    "                    [\n",
    "                        # NOTE(seungjaeryanlee): The success state in hint-to-goal is not relaxed.\n",
    "                        # TODO(seungjaeryanlee): What is goal velocity?\n",
    "                        np.random.uniform(-self.pos_success_range, self.pos_success_range),\n",
    "                        0,#np.random.uniform(-0.02, 0.02),\n",
    "                        0,\n",
    "                        np.random.randint(2),\n",
    "                        np.random.randint(2),\n",
    "                    ]\n",
    "                )\n",
    "                for _ in range(size)\n",
    "            ]\n",
    "            goal_target_q_values = np.zeros(size)\n",
    "\n",
    "            return goal_state_action_b, goal_target_q_values\n",
    "\n",
    "        def generate_rollout(\n",
    "            self, get_best_action: Callable = None, render: bool = False\n",
    "        ) -> List[Tuple[np.array, int, int, np.array, bool]]:\n",
    "            \"\"\"\n",
    "            Generate rollout using given action selection function.\n",
    "\n",
    "            If a network is not given, generate random rollout instead.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            get_best_action : Callable\n",
    "                Greedy policy.\n",
    "            render: bool\n",
    "                If true, render environment.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            rollout : List of Tuple\n",
    "                Generated rollout.\n",
    "            episode_cost : float\n",
    "                Cumulative cost throughout the episode.\n",
    "\n",
    "            \"\"\"\n",
    "            rollout = []\n",
    "            episode_cost = 0\n",
    "            obs = self.reset()\n",
    "            done = False\n",
    "            info = {\"time_limit\": False}\n",
    "            while not done and not info[\"time_limit\"]:\n",
    "                if get_best_action:\n",
    "                    action = get_best_action(obs)\n",
    "                else:\n",
    "                    action = self.action_space.sample()\n",
    "\n",
    "                next_obs, cost, done, info = self.step(action)\n",
    "                rollout.append((obs, action, cost, next_obs, done))\n",
    "                episode_cost += cost\n",
    "                obs = next_obs\n",
    "\n",
    "                if render:\n",
    "                    self.render()\n",
    "\n",
    "            return rollout, episode_cost\n",
    "\n",
    "    return SteerboxEnv\n",
    "\n",
    "SteerboxEnv = import_SteerboxEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_NFQAgent():\n",
    "    \"\"\"Reinforcement learning agents.\"\"\"\n",
    "    from typing import List, Tuple\n",
    "\n",
    "    import gym\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "\n",
    "\n",
    "    class NFQAgent:\n",
    "        def __init__(self, nfq_net: nn.Module, optimizer: optim.Optimizer):\n",
    "            \"\"\"\n",
    "            Neural Fitted Q-Iteration agent.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            nfq_net : nn.Module\n",
    "                The Q-Network that returns estimated cost given observation and action.\n",
    "            optimizer : optim.Optimzer\n",
    "                Optimizer for training the NFQ network.\n",
    "\n",
    "            \"\"\"\n",
    "            self._nfq_net = nfq_net\n",
    "            self._optimizer = optimizer\n",
    "\n",
    "        def get_best_action(self, obs: np.array) -> int:\n",
    "            \"\"\"\n",
    "            Return best action for given observation according to the neural network.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            obs : np.array\n",
    "                An observation to find the best action for.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            action : int\n",
    "                The action chosen by greedy selection.\n",
    "\n",
    "            \"\"\"\n",
    "            #import time\n",
    "            #s = time.monotonic()\n",
    "            q_left = self._nfq_net(\n",
    "                torch.cat([torch.FloatTensor(obs), torch.FloatTensor([0])], dim=0)\n",
    "            )\n",
    "            q_right = self._nfq_net(\n",
    "                torch.cat([torch.FloatTensor(obs), torch.FloatTensor([1])], dim=0)\n",
    "            )\n",
    "            \n",
    "            #print((time.monotonic()-s)*1000)\n",
    "\n",
    "            # Best action has lower \"Q\" value since it estimates cumulative cost.\n",
    "            return 1 if q_left >= q_right else 0\n",
    "\n",
    "        def generate_pattern_set(\n",
    "            self,\n",
    "            rollouts: List[Tuple[np.array, int, int, np.array, bool]],\n",
    "            gamma: float = 0.95,\n",
    "        ):\n",
    "            \"\"\"Generate pattern set.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            rollouts : list of tuple\n",
    "                Generated rollouts, which is a tuple of state, action, cost, next state, and done.\n",
    "            gamma : float\n",
    "                Discount factor. Defaults to 0.95.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            pattern_set : tuple of torch.Tensor\n",
    "                Pattern set to train the NFQ network.\n",
    "\n",
    "            \"\"\"\n",
    "            # _b denotes batch\n",
    "            state_b, action_b, cost_b, next_state_b, done_b = zip(*rollouts)\n",
    "            \n",
    "            state_b = torch.FloatTensor(state_b)\n",
    "            action_b = torch.FloatTensor(action_b)\n",
    "            \n",
    "            cost_b = torch.FloatTensor(cost_b)\n",
    "            next_state_b = torch.FloatTensor(next_state_b)\n",
    "            done_b = torch.FloatTensor(done_b)\n",
    "\n",
    "            state_action_b = torch.cat([state_b, action_b.unsqueeze(1)], 1)\n",
    "            assert state_action_b.shape == (len(rollouts), state_b.shape[1] + 1)\n",
    "\n",
    "            # Compute min_a Q(s', a)\n",
    "            q_next_state_left_b = self._nfq_net(\n",
    "                torch.cat([next_state_b, torch.zeros(len(rollouts), 1)], 1)\n",
    "            ).squeeze()\n",
    "            q_next_state_right_b = self._nfq_net(\n",
    "                torch.cat([next_state_b, torch.ones(len(rollouts), 1)], 1)\n",
    "            ).squeeze()\n",
    "            q_next_state_b = torch.min(q_next_state_left_b, q_next_state_right_b)\n",
    "\n",
    "            # If goal state (S+): target = 0 + gamma * min Q\n",
    "            # If forbidden state (S-): target = 1\n",
    "            # If neither: target = c_trans + gamma * min Q\n",
    "            # NOTE(seungjaeryanlee): done is True only when the episode terminated\n",
    "            #                        due to entering forbidden state. It is not\n",
    "            #                        True if it terminated due to maximum timestep.\n",
    "            with torch.no_grad():\n",
    "                target_q_values = cost_b + gamma * q_next_state_b * (1 - done_b)\n",
    "\n",
    "            return state_action_b, target_q_values\n",
    "\n",
    "        def train(self, pattern_set: Tuple[torch.Tensor, torch.Tensor]) -> float:\n",
    "            \"\"\"Train neural network with a given pattern set.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            pattern_set : tuple of torch.Tensor\n",
    "                Pattern set to train the NFQ network.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            loss : float\n",
    "                Training loss.\n",
    "\n",
    "            \"\"\"\n",
    "            state_action_b, target_q_values = pattern_set\n",
    "            for _ in range(300):\n",
    "                predicted_q_values = self._nfq_net(state_action_b).squeeze()\n",
    "                loss = F.mse_loss(predicted_q_values, target_q_values)\n",
    "\n",
    "                self._optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self._optimizer.step()\n",
    "\n",
    "            return loss.item()\n",
    "\n",
    "        def evaluate(self, eval_env: gym.Env, render: bool) -> Tuple[int, str, float]:\n",
    "            \"\"\"Evaluate NFQ agent on evaluation environment.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            eval_env : gym.Env\n",
    "                Environment to evaluate the agent.\n",
    "            render: bool\n",
    "                If true, render environment.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            episode_length : int\n",
    "                Number of steps the agent took.\n",
    "            success : bool\n",
    "                True if the agent was terminated due to max timestep.\n",
    "            episode_cost : float\n",
    "                Total cost accumulated from the evaluation episode.\n",
    "\n",
    "            \"\"\"\n",
    "            episode_length = 0\n",
    "            obs = eval_env.reset()\n",
    "            done = False\n",
    "            info = {\"time_limit\": False}\n",
    "            episode_cost = 0\n",
    "            while not done and not info[\"time_limit\"]:\n",
    "                action = self.get_best_action(obs)\n",
    "                obs, cost, done, info = eval_env.step(action)\n",
    "                episode_cost += cost\n",
    "                episode_length += 1\n",
    "\n",
    "                if render:\n",
    "                    eval_env.render()\n",
    "\n",
    "            success = (\n",
    "                episode_length == eval_env.max_steps\n",
    "                and abs(obs[0]) <= eval_env.pos_success_range\n",
    "                and abs(obs[1]) <= 0.01\n",
    "            )\n",
    "\n",
    "            return episode_length, success, episode_cost\n",
    "\n",
    "    return NFQAgent\n",
    "\n",
    "NFQAgent = import_NFQAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\"\"\"Networks for NFQ.\"\"\"\n",
    "\"\"\"\n",
    "def import_NFQNetwork():\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "\n",
    "    class NFQNetwork(nn.Module):\n",
    "        def __init__(self):\n",
    "            \n",
    "            \"\"\"Networks for NFQ.\"\"\"\n",
    "            super().__init__()\n",
    "            \n",
    "            self.layers = nn.Sequential(\n",
    "                \n",
    "                nn.Linear(5, 5),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(5, 5),\n",
    "                nn.Sigmoid(),\n",
    "                nn.Linear(5, 1),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "\n",
    "            # Initialize weights to [-0.5, 0.5]\n",
    "            def init_weights(m):\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.uniform_(m.weight, -0.5, 0.5)\n",
    "                    # TODO(seungjaeryanlee): What about bias?\n",
    "\n",
    "            self.layers.apply(init_weights)\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            \n",
    "            return self.layers(x)\n",
    "    \n",
    "    return NFQNetwork\n",
    "\n",
    "NFQNetwork = import_NFQNetwork()\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Forward propagation.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "x : torch.Tensor\n",
    "    Input tensor of observation and action concatenated.\n",
    "\n",
    "Returns\n",
    "-------\n",
    "y : torch.Tensor\n",
    "    Forward-propagated observation predicting Q-value.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "EPOCH = 500\n",
    "TRAIN_ENV_MAX_STEPS = 100\n",
    "EVAL_ENV_MAX_STEPS = 3000\n",
    "DISCOUNT = 0.95\n",
    "INIT_EXPERIENCE = 1\n",
    "RANDOM_SEED = 4 # does not actually matter...\n",
    "\n",
    "INCREMENT_EXPERIENCE = True\n",
    "HINT_TO_GOAL = True\n",
    "\n",
    "env = SteerboxEnv(mode=\"train\")\n",
    "\n",
    "if RANDOM_SEED is not None: # does not actually matter...\n",
    "    random.seed(RANDOM_SEED ^ 0xdf9b89026423c)\n",
    "    s = random.randrange(2**32)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    #train_env.seed(s)\n",
    "    #eval_env.seed(s)\n",
    "\n",
    "# Setup agent\n",
    "nfq_net = NFQNetwork()\n",
    "optimizer = optim.Rprop(nfq_net.parameters())\n",
    "nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "# NFQ Main loop\n",
    "# A set of transition samples denoted as D\n",
    "all_rollouts = []\n",
    "total_cost = 0\n",
    "if INIT_EXPERIENCE:\n",
    "    for _ in range(INIT_EXPERIENCE):\n",
    "        rollout, episode_cost = env.generate_rollout(\n",
    "            None, render=False\n",
    "        )\n",
    "        all_rollouts.extend(rollout)\n",
    "        total_cost += episode_cost\n",
    "for epoch in range(EPOCH + 1):\n",
    "    # Variant 1: Incermentally add transitions (Section 3.4)\n",
    "    # TODO(seungjaeryanlee): Done before or after training?\n",
    "    \n",
    "    env.set_mode(\"train\")\n",
    "\n",
    "    state_action_b, target_q_values = nfq_agent.generate_pattern_set(all_rollouts)\n",
    "\n",
    "    # Variant 2: Clamp function to zero in goal region\n",
    "    # TODO(seungjaeryanlee): Since this is a regulator setting, should it\n",
    "    #                        not be clamped to zero?\n",
    "    if HINT_TO_GOAL:\n",
    "        goal_state_action_b, goal_target_q_values = env.get_goal_pattern_set()\n",
    "        goal_state_action_b = torch.FloatTensor(goal_state_action_b)\n",
    "        goal_target_q_values = torch.FloatTensor(goal_target_q_values)\n",
    "        state_action_b = torch.cat([state_action_b, goal_state_action_b], dim=0)\n",
    "        target_q_values = torch.cat([target_q_values, goal_target_q_values], dim=0)\n",
    "\n",
    "    nfq_net = NFQNetwork()\n",
    "    optimizer = optim.Rprop(nfq_net.parameters())\n",
    "    nfq_agent = NFQAgent(nfq_net, optimizer)\n",
    "\n",
    "    env.reset()\n",
    "    loss = nfq_agent.train((state_action_b, target_q_values))\n",
    "\n",
    "    if INCREMENT_EXPERIENCE:\n",
    "        new_rollout, episode_cost = env.generate_rollout(\n",
    "            nfq_agent.get_best_action, render=False\n",
    "        )\n",
    "        all_rollouts.extend(new_rollout)\n",
    "        total_cost += episode_cost\n",
    "    \n",
    "    env.set_mode(\"eval\")\n",
    "\n",
    "    # TODO(seungjaeryanlee): Evaluation should be done with 3000 episodes\n",
    "    num_evals = 0\n",
    "    while num_evals < 50:\n",
    "        eval_episode_length, eval_success, eval_episode_cost = nfq_agent.evaluate(\n",
    "            env, False\n",
    "        )\n",
    "        if not eval_success: break\n",
    "        print(eval_episode_cost)\n",
    "        num_evals += 1\n",
    "\n",
    "    if INCREMENT_EXPERIENCE:\n",
    "        print(\n",
    "            \"Epoch {:4d} | Train {:3d} / {:4.2f} | Eval {:4d} / {:5.2f} | Train Loss {:.4f}\".format(  # noqa: B950\n",
    "                epoch,\n",
    "                len(new_rollout),\n",
    "                episode_cost,\n",
    "                eval_episode_length,\n",
    "                eval_episode_cost,\n",
    "                loss,\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Epoch {:4d} | Eval {:4d} / {:5.2f} | Train Loss {:.4f}\".format(\n",
    "                epoch, eval_episode_length, eval_episode_cost, loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if num_evals > 0:\n",
    "        print(\n",
    "            \"Epoch {:4d} | Total Cycles {:6d} | Total Cost {:4.2f} | Times {}\".format(\n",
    "                epoch, len(all_rollouts), total_cost, num_evals\n",
    "            )\n",
    "        )\n",
    "        if num_evals == 5: break\n",
    "\n",
    "#env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
